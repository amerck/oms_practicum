# Project Training Tests

## Potential Base LLM Models
* **deepseek-r1**: [https://ollama.com/library/deepseek-r1](https://ollama.com/library/deepseek-r1)
* **gemma3**: [https://ollama.com/library/gemma3](https://ollama.com/library/gemma3)
* **qwen3**: [https://ollama.com/library/qwen3](https://ollama.com/library/qwen3)
* **llama4**: [https://ollama.com/library/llama4](https://ollama.com/library/llama4)
* **llama3**: [https://ollama.com/library/llama3.3](https://ollama.com/library/llama3.3)


## Sentence Transformers

* This project will be using SBERT Pretrained Models for Sentence Transformers, found here: [https://www.sbert.net/docs/sentence_transformer/pretrained_models.html](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)
* Potential models:
    * all-mpnet-base-v2: "All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs."
    * multi-qa-mpnet-base-dot-v1: "This model was tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs."
    * all-distilroberta-v1: "All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs."
    * multi-qa-distilbert-cos-v1: "This model was tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs."